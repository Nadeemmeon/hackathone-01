# Data Model: Vision-Language-Action (VLA) Integration

## Core Entities

### VoiceCommand
Represents a voice command captured from a user.

**Fields**:
- id (string): Unique identifier for the command
- content (string): The text content from speech recognition
- confidence (number): Speech recognition confidence score (0.0 to 1.0)
- timestamp (datetime): When the command was received
- user_id (string): Identifier for the user who issued the command
- audio_metadata (object): Additional metadata about the audio input

**Relationships**:
- One VoiceCommand → One CognitivePlan

### CognitivePlan
Represents a structured plan generated by the LLM from a voice command.

**Fields**:
- id (string): Unique identifier for the plan
- original_command_id (string): Reference to the original voice command
- original_command_text (string): The original voice command text
- task_sequence (array): Ordered list of robot actions to execute
- status (string): Current status (pending, in_progress, completed, failed)
- created_at (datetime): When the plan was created
- completed_at (datetime): When the plan was completed (if applicable)

**Relationships**:
- One CognitivePlan → Many RobotAction
- One CognitivePlan → One VoiceCommand

### RobotAction
Represents a specific action to be executed by the robot.

**Fields**:
- id (string): Unique identifier for the action
- plan_id (string): Reference to the parent cognitive plan
- action_type (string): Type of action (navigation, manipulation, perception, etc.)
- parameters (object): Specific parameters for the action
- sequence_number (number): Position in the task sequence
- status (string): Execution status (pending, in_progress, completed, failed)
- error_message (string): Error details if action failed
- executed_at (datetime): When the action was executed (if applicable)

**Relationships**:
- One RobotAction → One CognitivePlan

### PerceptionData
Represents data gathered by robot sensors during task execution.

**Fields**:
- id (string): Unique identifier for the perception data
- action_id (string): Reference to the action that triggered perception
- sensor_type (string): Type of sensor (camera, lidar, etc.)
- data (object): The actual perception data
- timestamp (datetime): When the data was captured
- context (string): Context of the perception (object detection, navigation, etc.)

**Relationships**:
- One PerceptionData → One RobotAction

## State Transitions

### CognitivePlan Status Transitions
```
pending → in_progress → completed
              │              │
              └──→ failed ←──┘
```

### RobotAction Status Transitions
```
pending → in_progress → completed
              │              │
              └──→ failed ←──┘
```

## Validation Rules

1. **VoiceCommand**:
   - Content must not be empty
   - Confidence must be between 0.0 and 1.0
   - Timestamp must be within reasonable bounds

2. **CognitivePlan**:
   - Must have at least one task in the sequence
   - Task sequence must form a valid execution plan
   - Status transitions must follow defined rules

3. **RobotAction**:
   - Action type must be one of predefined types
   - Parameters must match the expected schema for the action type
   - Sequence numbers must be unique within a plan

## Data Flow

1. VoiceCommand is created when user speaks
2. CognitivePlan is generated from VoiceCommand
3. RobotActions are created from CognitivePlan task sequence
4. PerceptionData is generated during RobotAction execution
5. Status updates flow back to update parent entities